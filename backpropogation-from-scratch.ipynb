{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation from Scratch \n",
    "\n",
    "#### Final Project for COMPSCIX404 - Data Structures and Algorithms\n",
    "#### Nikhil Gopal \n",
    "#### August 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will build the backpropogation algorithm, and show how it can be used to compute the gradient of the cost function when training a Neural Network. Neural Networks trained with backpropogation can be applied to text generation, computer vision, classification, facial recognition, audio generation, speech to text and many more applications.\n",
    "\n",
    "I will start by explaining the algorithm and it's applicability to modern Deep Learning, with an overview of the mathematics behind it. Then, I will analyze the time and space complexity of the algorithm before presenting a code implementation.\n",
    "\n",
    "Finally, we'll use the Pytorch Deep Learning Framework to show a real life example of how a Data Scientist or Deep Learning Engineer would train a Neural Network to classify handwritten digits from the popular MNSIT dataset. Pytorch automatically computes derivatives and gradients for you with the `loss.backwards()` method, but I will still show it so that you get an understanding of how this would be applied in the real world. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, every problem in Machine Learning is an **optimization problem**. Optimization is the process of finding the maximum or minimum value of a function, such that it behaves optimally. In the case of Machine Learning, we seek to optimize the *loss function*, which describes the difference between the expected output of an ML model, and it's actual output. For example, if we were predicting human height and our model predicted 100 cm for a person who's true height was 105 cm, the loss would be $105-100=5$ cm. ML models have changable *parameters* whose optimal values can be learned from data such that the model's loss function is as close as possible to zero. \n",
    "\n",
    "\n",
    "Deep Learning is a subsect of Machine Learning that uses a specific type of Machine Learning model called a Multi-Layer Perceptron, also commonly referred to as Neural Networks. Neural Networks pass an input through different \"layers\" of the model, which all contribute to the model's final output. These models often have much higher numbers of learnable parameters than simpler models, but the computations corresponding to a single input must be performed sequentially and cannot be parallelized, since the input of a later layer must be the output of a previous layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input image depicting Neural Net here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many advancements in Machine Learning have come from using Deep Neural Networks (models with lots of layers and parameters) to learn the relevant features of and transform inputted data in novel ways. By leveraging techniques from Calculus, we can compute the gradient vector of a Neural Network's cost function. Each index of the gradient vector contains the partial derivative of the cost function with respect to a parameter, which describes the impact that each individual parameter has on the total loss of the network. The sum of all the loss that comes from every input parameter is the total loss of the network, thus making computing the gradient a key factor in minimizing the loss function.\n",
    "\n",
    "Once the gradient is computed, the model's paramaters are adjusted using a parameter estimation method such as gradient descent, and the loss and parameters are continously adjusted and recomputed until the model's behavior more closely mimics the information it learned from the training data.\n",
    "\n",
    "As model sizes keep increasing, an efficient algorithm is needed to compute the gradient. This algorithm should ideally be computationally efficient so as to minimize total training time and memory usage when training neural networks. This will ensure that models can be deployed into producition faster, and that they can be made more accuracte by being able to be trained on more data in less time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math explaining in detail here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to network: [[0 0]]\n",
      "Layer 0 output: [[0.5 0.5 0.5 0.5]]\n",
      "Layer 1 output: [[0.50074883 0.49790639 0.49832571 0.50156196]]\n",
      "Layer 2 output: [[0.49908172 0.50361903 0.4959061  0.50294069]]\n",
      "Layer 3 output: [[0.50112136]]\n",
      "Final output: [[0.50112136]]\n",
      "Output error: [[-0.50112136]]\n",
      "Layer 3 - Loss wrt Layer: w_3, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_3), gradients used from previous layers: [dloss/dw_3: [[-0.06252481]\n",
      " [-0.06309325]\n",
      " [-0.06212697]\n",
      " [-0.06300826]], dloss/db_3: [[-0.12527971]]]\n",
      "Error propagated to Layer 2: [[-0.00201579 -0.00092231  0.00124345  0.00058052]]\n",
      "Layer 2 - Loss wrt Layer: w_2, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_2), gradients used from previous layers: [dloss/dw_2: [[-2.52350371e-04 -1.15455731e-04  1.55653690e-04  7.26711207e-05]\n",
      " [-2.50917937e-04 -1.14800361e-04  1.54770141e-04  7.22586123e-05]\n",
      " [-2.51129248e-04 -1.14897041e-04  1.54900482e-04  7.23194652e-05]\n",
      " [-2.52760146e-04 -1.15643212e-04  1.55906445e-04  7.27891263e-05]], dloss/db_2: [[-0.00050395 -0.00023057  0.00031084  0.00014512]]]\n",
      "Error propagated to Layer 1: [[-5.11257413e-07  8.23690381e-06 -6.42016439e-06 -1.09972793e-05]]\n",
      "Layer 1 - Loss wrt Layer: w_1, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_1), gradients used from previous layers: [dloss/dw_1: [[-6.39070333e-08  1.02959492e-06 -8.02511550e-07 -1.37464650e-06]\n",
      " [-6.39070333e-08  1.02959492e-06 -8.02511550e-07 -1.37464650e-06]\n",
      " [-6.39070333e-08  1.02959492e-06 -8.02511550e-07 -1.37464650e-06]\n",
      " [-6.39070333e-08  1.02959492e-06 -8.02511550e-07 -1.37464650e-06]], dloss/db_1: [[-1.27814067e-07  2.05918985e-06 -1.60502310e-06 -2.74929299e-06]]]\n",
      "Error propagated to Layer 0: [[ 5.73166815e-09  2.24322075e-08 -8.41347020e-08  7.85976731e-09]]\n",
      "Layer 0 - Loss wrt Layer: w_0, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_0), gradients used from previous layers: [dloss/dw_0: [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], dloss/db_0: [[ 1.43291704e-09  5.60805188e-09 -2.10336755e-08  1.96494183e-09]]]\n",
      "Error propagated to Layer -1: [[-6.61237140e-11  9.98301379e-11]]\n",
      "Input to network: [[0 1]]\n",
      "Layer 0 output: [[0.50026595 0.50137875 0.49907597 0.4986812 ]]\n",
      "Layer 1 output: [[0.50074137 0.49790214 0.49832748 0.50154904]]\n",
      "Layer 2 output: [[0.49908421 0.50362014 0.49590455 0.50293997]]\n",
      "Layer 3 output: [[0.50174827]]\n",
      "Final output: [[0.50174827]]\n",
      "Output error: [[0.49825173]]\n",
      "Layer 3 - Loss wrt Layer: w_3, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_3), gradients used from previous layers: [dloss/dw_3: [[0.06216663]\n",
      " [0.06273163]\n",
      " [0.06177057]\n",
      " [0.06264691]], dloss/db_3: [[0.12456141]]]\n",
      "Error propagated to Layer 2: [[ 0.00208211  0.00099561 -0.00115894 -0.00049871]]\n",
      "Layer 2 - Loss wrt Layer: w_2, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_2), gradients used from previous layers: [dloss/dw_2: [[ 2.60649394e-04  1.24629823e-04 -1.45071952e-04 -6.24286756e-05]\n",
      " [ 2.59171499e-04  1.23923166e-04 -1.44249387e-04 -6.20747019e-05]\n",
      " [ 2.59392898e-04  1.24029028e-04 -1.44372613e-04 -6.21277297e-05]\n",
      " [ 2.61069807e-04  1.24830844e-04 -1.45305945e-04 -6.25293695e-05]], dloss/db_2: [[ 0.00052053  0.00024889 -0.00028971 -0.00012467]]]\n",
      "Error propagated to Layer 1: [[ 9.23333168e-07 -8.68535047e-06  6.43161872e-06  1.12254986e-05]]\n",
      "Layer 1 - Loss wrt Layer: w_1, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_1), gradients used from previous layers: [dloss/dw_1: [[ 1.15477783e-07 -1.08622716e-06  8.04370968e-07  1.40392021e-06]\n",
      " [ 1.15734652e-07 -1.08864337e-06  8.06160213e-07  1.40704310e-06]\n",
      " [ 1.15203096e-07 -1.08364336e-06  8.02457612e-07  1.40058071e-06]\n",
      " [ 1.15111969e-07 -1.08278618e-06  8.01822857e-07  1.39947283e-06]], dloss/db_1: [[ 2.30832785e-07 -2.17129939e-06  1.60788669e-06  2.80634771e-06]]]\n",
      "Error propagated to Layer 0: [[-6.14142524e-09 -2.23723447e-08  8.55617294e-08 -5.76715470e-09]]\n",
      "Layer 0 - Loss wrt Layer: w_0, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_0), gradients used from previous layers: [dloss/dw_0: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.53535588e-09 -5.59304365e-09  2.13903593e-08 -1.44177865e-09]], dloss/db_0: [[-1.53535588e-09 -5.59304365e-09  2.13903593e-08 -1.44177865e-09]]]\n",
      "Error propagated to Layer -1: [[ 6.66239778e-11 -1.03934492e-10]]\n",
      "Input to network: [[1 0]]\n",
      "Layer 0 output: [[0.49938349 0.49611595 0.49974598 0.50040296]]\n",
      "Layer 1 output: [[0.5007551  0.49791991 0.49832274 0.50157954]]\n",
      "Layer 2 output: [[0.49908163 0.503619   0.49590598 0.50294056]]\n",
      "Layer 3 output: [[0.50112496]]\n",
      "Final output: [[0.50112496]]\n",
      "Output error: [[0.49887504]]\n",
      "Layer 3 - Loss wrt Layer: w_3, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_3), gradients used from previous layers: [dloss/dw_3: [[0.06224453]\n",
      " [0.06281042]\n",
      " [0.06184847]\n",
      " [0.06272581]], dloss/db_3: [[0.12471813]]]\n",
      "Error propagated to Layer 2: [[ 0.0020072   0.00091863 -0.00123743 -0.00057747]]\n",
      "Layer 2 - Loss wrt Layer: w_2, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_2), gradients used from previous layers: [dloss/dw_2: [[ 2.51278251e-04  1.14996083e-04 -1.54902251e-04 -7.22898528e-05]\n",
      " [ 2.49855555e-04  1.14344995e-04 -1.54025221e-04 -7.18805598e-05]\n",
      " [ 2.50057693e-04  1.14437502e-04 -1.54149830e-04 -7.19387126e-05]\n",
      " [ 2.51691953e-04  1.15185412e-04 -1.55157281e-04 -7.24088701e-05]], dloss/db_2: [[ 0.0005018   0.00022965 -0.00030934 -0.00014436]]]\n",
      "Error propagated to Layer 1: [[ 5.11320530e-07 -8.20285223e-06  6.39163367e-06  1.09496252e-05]]\n",
      "Layer 1 - Loss wrt Layer: w_1, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_1), gradients used from previous layers: [dloss/dw_1: [[ 6.38361122e-08 -1.02407452e-06  7.97960103e-07  1.36700187e-06]\n",
      " [ 6.34184235e-08 -1.01737386e-06  7.92738937e-07  1.35805738e-06]\n",
      " [ 6.38824486e-08 -1.02481786e-06  7.98539315e-07  1.36799413e-06]\n",
      " [ 6.39664303e-08 -1.02616512e-06  7.99589097e-07  1.36979253e-06]], dloss/db_1: [[ 1.27829841e-07 -2.05067757e-06  1.59789044e-06  2.73737898e-06]]]\n",
      "Error propagated to Layer 0: [[-5.70839797e-09 -2.23317998e-08  8.37683003e-08 -7.81286531e-09]]\n",
      "Layer 0 - Loss wrt Layer: w_0, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_0), gradients used from previous layers: [dloss/dw_0: [[-1.42709732e-09 -5.58261306e-09  2.09420697e-08 -1.95321506e-09]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]], dloss/db_0: [[-1.42709732e-09 -5.58261306e-09  2.09420697e-08 -1.95321506e-09]]]\n",
      "Error propagated to Layer -1: [[ 6.58261140e-11 -9.94069322e-11]]\n",
      "Input to network: [[1 1]]\n",
      "Layer 0 output: [[0.49964944 0.49749465 0.49882195 0.49908415]]\n",
      "Layer 1 output: [[0.50074764 0.49791568 0.49832449 0.50156659]]\n",
      "Layer 2 output: [[0.4990791  0.50361781 0.49590753 0.50294129]]\n",
      "Layer 3 output: [[0.50050086]]\n",
      "Final output: [[0.50050086]]\n",
      "Output error: [[-0.50050086]]\n",
      "Layer 3 - Loss wrt Layer: w_3, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_3), gradients used from previous layers: [dloss/dw_3: [[-0.06244732]\n",
      " [-0.06301522]\n",
      " [-0.06205047]\n",
      " [-0.06293057]], dloss/db_3: [[-0.12512509]]]\n",
      "Error propagated to Layer 2: [[-0.00193587 -0.00084304  0.00131886  0.00065784]]\n",
      "Layer 2 - Loss wrt Layer: w_2, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_2), gradients used from previous layers: [dloss/dw_2: [[-2.42344455e-04 -1.05531481e-04  1.65092704e-04  8.23497000e-05]\n",
      " [-2.40973884e-04 -1.04934651e-04  1.64159028e-04  8.18839741e-05]\n",
      " [-2.41171735e-04 -1.05020807e-04  1.64293810e-04  8.19512048e-05]\n",
      " [-2.42740798e-04 -1.05704072e-04  1.65362705e-04  8.24843789e-05]], dloss/db_2: [[-0.00048397 -0.00021075  0.00032969  0.00016445]]]\n",
      "Error propagated to Layer 1: [[-9.80346231e-08  7.73387226e-06 -6.36428559e-06 -1.06941366e-05]]\n",
      "Layer 1 - Loss wrt Layer: w_1, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_1), gradients used from previous layers: [dloss/dw_1: [[-1.22457088e-08  9.66039456e-07 -7.94969011e-07 -1.33581674e-06]\n",
      " [-1.21928978e-08  9.61873298e-07 -7.91540615e-07 -1.33005587e-06]\n",
      " [-1.22254281e-08  9.64439549e-07 -7.93652423e-07 -1.33360443e-06]\n",
      " [-1.22318543e-08  9.64946497e-07 -7.94069599e-07 -1.33430542e-06]], dloss/db_1: [[-2.45086010e-08  1.93343447e-06 -1.59105353e-06 -2.67350791e-06]]]\n",
      "Error propagated to Layer 0: [[ 5.28466050e-09  2.23364432e-08 -8.21326281e-08  9.88576157e-09]]\n",
      "Layer 0 - Loss wrt Layer: w_0, Equation of derivative: (sigmoid_derivative(output) * d_output * dloss/dw_0), gradients used from previous layers: [dloss/dw_0: [[ 1.32116448e-09  5.58397060e-09 -2.05330430e-08  2.47143210e-09]\n",
      " [ 1.32116448e-09  5.58397060e-09 -2.05330430e-08  2.47143210e-09]], dloss/db_0: [[ 1.32116448e-09  5.58397060e-09 -2.05330430e-08  2.47143210e-09]]]\n",
      "Error propagated to Layer -1: [[-6.51663030e-11  9.50562044e-11]]\n",
      "Input to network: [[0 0]]\n",
      "Layer 0 output: [[0.5 0.5 0.5 0.5]]\n",
      "Layer 1 output: [[0.50074883 0.49790639 0.49832571 0.50156196]]\n",
      "Layer 2 output: [[0.49908155 0.50361885 0.49590589 0.50294049]]\n",
      "Layer 3 output: [[0.50112699]]\n",
      "Final output: [[0.50112699]]\n",
      "Input: [0 0], Predicted Output: [[0.50112699]], Target: [0]\n",
      "Input to network: [[0 1]]\n",
      "Layer 0 output: [[0.50026595 0.50137875 0.49907597 0.4986812 ]]\n",
      "Layer 1 output: [[0.50074137 0.49790215 0.49832747 0.50154902]]\n",
      "Layer 2 output: [[0.49908152 0.5036188  0.4959059  0.50294049]]\n",
      "Layer 3 output: [[0.50112699]]\n",
      "Final output: [[0.50112699]]\n",
      "Input: [0 1], Predicted Output: [[0.50112699]], Target: [1]\n",
      "Input to network: [[1 0]]\n",
      "Layer 0 output: [[0.49938349 0.49611595 0.49974598 0.50040296]]\n",
      "Layer 1 output: [[0.5007551  0.49791991 0.49832274 0.50157954]]\n",
      "Layer 2 output: [[0.49908154 0.50361891 0.49590588 0.50294046]]\n",
      "Layer 3 output: [[0.50112699]]\n",
      "Final output: [[0.50112699]]\n",
      "Input: [1 0], Predicted Output: [[0.50112699]], Target: [1]\n",
      "Input to network: [[1 1]]\n",
      "Layer 0 output: [[0.49964944 0.49749465 0.49882195 0.49908415]]\n",
      "Layer 1 output: [[0.50074764 0.49791567 0.4983245  0.5015666 ]]\n",
      "Layer 2 output: [[0.49908151 0.50361886 0.49590588 0.50294047]]\n",
      "Layer 3 output: [[0.50112699]]\n",
      "Final output: [[0.50112699]]\n",
      "Input: [1 1], Predicted Output: [[0.50112699]], Target: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.bias\n",
    "        self.output = self.sigmoid(self.z)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output, layer_idx):\n",
    "        d_z = d_output * self.sigmoid_derivative(self.output)\n",
    "        d_weights = np.dot(self.inputs.T, d_z)\n",
    "        d_bias = np.sum(d_z, axis=0, keepdims=True)\n",
    "        d_inputs = np.dot(d_z, self.weights.T)\n",
    "\n",
    "        equation = f\"sigmoid_derivative(output) * d_output * dloss/dw_{layer_idx}\"\n",
    "        gradients = f\"dloss/dw_{layer_idx}: {d_weights}, dloss/db_{layer_idx}: {d_bias}\"\n",
    "\n",
    "        print(f\"Layer {layer_idx} - Loss wrt Layer: w_{layer_idx}, Equation of derivative: ({equation}), gradients used from previous layers: [{gradients}]\")\n",
    "\n",
    "        self.weights -= self.learning_rate * d_weights\n",
    "        self.bias -= self.learning_rate * d_bias\n",
    "\n",
    "        return d_inputs\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(Layer(layer_sizes[i], layer_sizes[i+1]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print(f\"Input to network: {inputs}\")\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            inputs = layer.forward(inputs)\n",
    "            print(f\"Layer {idx} output: {inputs}\")\n",
    "        self.output = inputs\n",
    "        print(f\"Final output: {self.output}\")\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, target):\n",
    "        error = target - self.output\n",
    "        d_output = error\n",
    "        print(f\"Output error: {error}\")\n",
    "        for idx, layer in enumerate(reversed(self.layers)):\n",
    "            d_output = layer.backward(d_output, len(self.layers) - 1 - idx)\n",
    "            print(f\"Error propagated to Layer {len(self.layers) - 2 - idx}: {d_output}\")\n",
    "\n",
    "    def train(self, inputs, target):\n",
    "        self.forward(inputs)\n",
    "        self.backward(target)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the network with 2 inputs, three hidden layers with 4 neurons each, and 1 output\n",
    "    nn = NeuralNetwork(layer_sizes=[2, 4, 4, 4, 1])\n",
    "    \n",
    "    # Training data: XOR problem\n",
    "    inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    targets = np.array([[0], [1], [1], [0]])\n",
    "    \n",
    "    # Training the neural network\n",
    "    epochs = 1\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(inputs)):\n",
    "            nn.train(inputs[i:i+1], targets[i:i+1])\n",
    "    \n",
    "    # Testing the neural network\n",
    "    for i in range(len(inputs)):\n",
    "        output = nn.forward(inputs[i:i+1])\n",
    "        print(f\"Input: {inputs[i]}, Predicted Output: {output}, Target: {targets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we define transformations that will turn our image datasets into tensors that pytorch can use, and normalize them, with mean $0.5$ and standard deviation $0.5$, so that values are approximately between the range $[-1,1]$. This is optimal for the allowing the gradient descent algorithm to converge to a true local minimum, and would be very necessary if you were using input features with different scales (though we aren't here).\n",
    "\n",
    "Next, we download training and test datasets, and apply our transformation onto them, and define our batch size. This helps improve computational efficiency by computing the gradient and loss of a batch of weights, and then adjusting the parameters based on a batch, instead of once for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define a Neural Network class, as a child class of PyTorch's nn.Module class. In the initialization, we'll first use the initialization of the parent function, and then create our layers. Each MNIST image is a 28x28 image. After applying the transformation, pytorch flattens this automatically  into a 1D 28*28 = 784 length vector in the input layer. \n",
    "\n",
    "The first hidden layer fc1 thus takes 28*28 input nodes, and outputs 128, which is the input size of the next hidden layer. We then reduce the dimensionality to 64 before passing through the second activation layer. Finally, we have 10 possible outputs, representing one of the ten possible digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create an instance of the neural net class, and then define our loss function and parameter estimation method. Cross entropy loss is a common loss function for categorical data, and stochastic gradient descent is a commonly used algorithm to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train our model over 5 epochs. Notice how the loss decreases with each iteration. This is the network learning in action. The model is using the computed gradient to calculate loss, and then adjust the weights using stochastic gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4150677062968201\n",
      "Epoch 2, Loss: 0.17772377693632455\n",
      "Epoch 3, Loss: 0.13109835688827007\n",
      "Epoch 4, Loss: 0.10235350166828727\n",
      "Epoch 5, Loss: 0.08720600267828528\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # Num of epochs\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test our model's accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 96.58 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test set: {100 * correct / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
