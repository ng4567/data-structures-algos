{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation from Scratch \n",
    "\n",
    "#### Final Project for COMPSCIX404 - Data Structures and Algorithms\n",
    "#### Nikhil Gopal \n",
    "#### August 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will build the backpropogation algorithm, and show how it can be used to compute the gradient of the cost function when training a Neural Network. Neural Networks trained with backpropogation can be applied to text generation, computer vision, classification, facial recognition, audio generation, speech to text and many more applications.\n",
    "\n",
    "I will start by explaining the algorithm and it's applicability to modern Deep Learning, with an overview of the mathematics behind it. We will discuss how Dynamic Programming techniques are leveraged to make the algorithm more efficient. Finally, I will analyze the time and space complexity of the algorithm before presenting a code implementation.\n",
    "\n",
    "Finally, we'll use the Pytorch Deep Learning Framework to show a real life example of how a Data Scientist or Deep Learning Engineer would train a Neural Network to classify handwritten digits from the popular MNSIT dataset. Pytorch automatically computes derivatives and gradients for you with the `loss.backwards()` method, but I will still show it so that you get an understanding of how this would be applied in the real world. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, every problem in Machine Learning is an **optimization problem**. Optimization is the process of finding the maximum or minimum value of a function, such that it behaves optimally. In the case of Machine Learning, we seek to optimize the *loss function*, which describes the difference between the expected output of an ML model, and it's actual output. For example, if we were predicting human height and our model predicted 100 cm for a person who's true height was 105 cm, the loss would be $105-100=5$ cm. ML models have changable *parameters* whose optimal values can be learned from data such that the model's loss function is as close as possible to zero. \n",
    "\n",
    "Deep Learning is a subsect of Machine Learning that relies on a specific model called a Neural Network. Neural Networks pass inputs through different \"layers\" of the model, which all contribute to the model's final output. These models often have much higher numbers of learnable parameters than simpler models, but the computations corresponding to a single input must be performed sequentially and cannot be parallelized, since the input of a later layer must be the output of a previous layer. Activation functions are applied to force a layer's output to fall within a specified range of values, to fall within a set of values, or to generate a probability distribution of values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](/Users/nikhilgopal/data-structures-algos/imgs/nn-img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many advancements in Machine Learning have come from using Deep Neural Networks (models with lots of layers and parameters) to learn the relevant features of and transform inputted data in novel ways. By leveraging techniques from Calculus, we can compute the gradient vector of a Neural Network's cost function. Each index of the gradient vector contains the partial derivative of the cost function with respect to a parameter, which describes the impact that each individual parameter has on the total loss of the network. The sum of all of the loss that comes from every input parameter is the total loss of the network, thus making computing the gradient a key factor in minimizing the loss function.\n",
    "\n",
    "Once the gradient is computed, the model's paramaters are adjusted using a parameter estimation method such as gradient descent, and the loss and parameters are continously adjusted and recomputed until the model's behavior more closely mimics the information it learned from the training data.\n",
    "\n",
    "As model sizes keep increasing, an efficient algorithm is needed to compute the gradient. This algorithm should ideally be computationally efficient so as to minimize total training time and memory usage when training neural networks. This will ensure that models can be deployed into producition faster, and that they can be made more accuracte by being able to be trained on more data in less time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we need to compute:\n",
    "\n",
    "$\\text{Loss} = L(x) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$\n",
    "\n",
    "Where $y_i$ is the true value for a given observation and $\\hat{y}_i$ is the value predicted by the model.\n",
    "\n",
    "We also know that the loss function is parameterized by the weights and biases of the network, denoted as $\\theta$:\n",
    "\n",
    " $\\text{Loss} = L(x; \\theta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute this loss, we must compute the gradient vector $\\nabla L(x)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla L = \\left( \\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\ldots, \\frac{\\partial L}{\\partial w_n}, \\frac{\\partial L}{\\partial b_1}, \\frac{\\partial L}{\\partial b_2}, \\ldots, \\frac{\\partial L}{\\partial b_m} \\right)$\n",
    "\n",
    "$\\nabla L(x)$ contains the partial derivatives of the weights and bias in the network. The different elements of this vector tell us the impact that changing each parameter by a unit of 1 will have on the total loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the partial derivatives we'll update the parameters using gradient descent: \n",
    "$w_i \\leftarrow w_i - \\eta \\frac{\\partial L}{\\partial w_i}$ where $\\eta$ is the learning rate of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging some clever math explained [here](http://neuralnetworksanddeeplearning.com/chap2.html), we arrive at these fundamental equations of backpropogation, which explain how to calculate the necessary partial derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/backprop-equations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (BP1):\n",
    "\n",
    " $\\delta^L = \\nabla_a C \\odot \\sigma{\\prime}(z^L)$\n",
    "\n",
    "-\t$\\delta^L$: This represents the error in the output layer L.\n",
    "-\t$\\nabla_a C$: This is the gradient of the cost function C with respect to the activations a in the output layer. It measures how the cost changes with the output activations.\n",
    "-\t$\\odot$: This denotes the Hadamard product (element-wise multiplication).\n",
    "-\t$\\sigma{\\prime}(z^L)$: This is the derivative of the activation function $\\sigma$ with respect to the input $z^L$ to the output layer.\n",
    "\n",
    "Explanation: This equation calculates the error at the output layer by combining the gradient of the cost function and the derivative of the activation function.\n",
    "\n",
    "Equation (BP2):\n",
    "\n",
    "$\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma{\\prime}(z^l)$\n",
    "\n",
    "-\t$\\delta^l$: This represents the error in layer $l$.\n",
    "-\t$w^{l+1}$: This is the weight matrix connecting layer $l$ to layer $l+1$.\n",
    "-\t$(w^{l+1})^T$: This is the transpose of the weight matrix $w^{l+1}$.\n",
    "-\t$\\delta^{l+1}$: This is the error in the next layer, $l+1$.\n",
    "-\t$\\sigma{\\prime}(z^l)$: This is the derivative of the activation function with respect to the input $z^l$ to layer $l$.\n",
    "\n",
    "Explanation: This equation calculates the error for layer $l$ by propagating the error from the next layer $l+1$ backwards through the network, adjusting for the activation function’s derivative.\n",
    "\n",
    "Equation (BP3):\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l$ \n",
    "\n",
    "-\t$\\frac{\\partial C}{\\partial b_j^l}$: This is the partial derivative of the cost function $C$ with respect to the bias $b_j^l$ in layer $l$.\n",
    "-\t$\\delta_j^l$: This is the error for neuron $j$ in layer $l$.\n",
    "\n",
    "Explanation: This equation states that the gradient of the cost function with respect to the bias is simply the error term $\\delta_j^l$.\n",
    "\n",
    "Equation (BP4):\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l$ \n",
    "\n",
    "-\t$\\frac{\\partial C}{\\partial w_{jk}^l}$: This is the partial derivative of the cost function C with respect to the weight $w_{jk}^l$ connecting neuron $k$ in layer $l-1$ to neuron $j$ in layer $l$.\n",
    "-\t$a_k^{l-1}$: This is the activation of neuron $k$ in the previous layer $l-1$.\n",
    "-\t$\\delta_j^l$: This is the error for neuron $j$ in layer $l$.\n",
    "\n",
    "Explanation: This equation calculates the gradient of the cost function with respect to the weights. It shows that this gradient is the product of the error term $\\delta_j^l$ and the activation of the neuron in the previous layer $a_k^{l-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having seen these equations, your data structures and algorithms knowledge should be sounding alarms in your brain that calculations can be reused to optimize the efficiency of our gradient calculuations!\n",
    "\n",
    "* Before any loss is calculated $W^T$ is initialized during the forward pass to calculate the prediction. This calculation should be saved and not redone during the loss calculation\n",
    "* In the second equation, we observe that the part of the computation for the loss of a given layer includes $\\sigma{\\prime}(z^l)$, which represents the loss of the next layer in the network. By starting at the end of the network and applying the chain rule to calculate the gradient backwards, we can reduce the total number of computations by reusing derivatives from previous layers! \n",
    "  * The above applies to individual neurons and not just layers as well (BP4)\n",
    "* By leveraging matrix multiplications, we can reduce runtime (not time complexity). The overall number of computations will be the same, but since multiplications in matrix math do not need to be performed sequentially, we can leverage GPUs and parallel processing to compute them in parallel and reduce overall algorithm runtime. \n",
    "\n",
    "Algorithm Pseudo code:\n",
    "\n",
    "```python\n",
    "def forward_pass(weights, biases):\n",
    "  activation_funcation(weight + bias)\n",
    "\n",
    "def backprop(weights):\n",
    "  \n",
    "  gradient = []\n",
    "\n",
    "  for x in range(output_layer:input_layer):  \n",
    "    if layer == final:\n",
    "      dloss = derivative(layer)\n",
    "      gradient[current_layer] = dloss\n",
    "    else:\n",
    "      dloss = derivative(current_layer)*gradient[layer+1]\n",
    "      gradient[current_layer] = dloss\n",
    "\n",
    "  return gradient\n",
    "```\n",
    "\n",
    "1. Initialize Gradient Arrays\n",
    "\n",
    "\t-\tCreate zero-filled arrays for the gradients of the biases ($nabla_b$) and weights ($nabla_w$).\n",
    "\t-\tThese arrays should match the shapes of the biases and weights in your network.\n",
    "\n",
    "2. Forward Pass\n",
    "\n",
    "\t-\tStart with the input activation ($a = x$).\n",
    "\t-\tFor each layer in the network:\n",
    "\t-\tCompute the weighted input $z$ as the dot product of the weights and the activation from the previous layer, plus the bias.\n",
    "\t-\tApply the activation function (e.g., sigmoid) to $z$ to get the new activation.\n",
    "\t-\tStore both $z$ and the new activation for use in the backward pass.\n",
    "\n",
    "3. Backward Pass for Output Layer\n",
    "\n",
    "\t-\tCompute the error term ($\\delta_L$) for the output layer:\n",
    "\t-\tCalculate the derivative of the cost function with respect to the output activations.\n",
    "\t-\tMultiply this by the derivative of the activation function evaluated at the weighted input $z$ of the output layer.\n",
    "\t-\tUpdate the gradients for the output layer’s biases and weights using this error term.\n",
    "\n",
    "4. Backward Pass for Hidden Layers\n",
    "\n",
    "\t-\tFor each hidden layer, moving backward:\n",
    "\t-\tCompute the error term ($\\delta$) for the current layer:\n",
    "\t-\tMultiply the transpose of the weights from the next layer by the error term from the next layer.\n",
    "\t-\tMultiply element-wise by the derivative of the activation function evaluated at the weighted input $z$ of the current layer.\n",
    "\t-\tUpdate the gradients for the current layer’s biases and weights using this error term.\n",
    "\n",
    "5. Calculate Gradients\n",
    "\n",
    "\t-\tFor each layer:\n",
    "\t-\tThe gradient for the biases is the error term ($\\delta$) itself.\n",
    "\t-\tThe gradient for the weights is the dot product of the error term ($\\delta$) and the transpose of the activations from the previous layer.\n",
    "\n",
    "6. Return Gradients\n",
    "\n",
    "\t-\tFinally, return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit to Michael Nielsen who wrote the code for this Neural Network class implementation in this tutorial: [link](https://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits)\n",
    "\n",
    "Since this assignment is focused specifically on backpropogation, I will write my own backpropogation implementation. To save time, the rest of the code I took from his git repo for the Python 3 implementation ([link](https://github.com/unexploredtest/neural-networks-and-deep-learning.git))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/unexploredtest/neural-networks-and-deep-learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('neural-networks-and-deep-learning/src')\n",
    "\n",
    "import mnist_loader\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            time1 = time.time()\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            time2 = time.time()\n",
    "            # if test_data:\n",
    "            #     print(\"Epoch {0}: {1} {2}, took {3:.2f} seconds\".format(\n",
    "            #         j, self.evaluate(test_data), n_test, time2-time1))\n",
    "            # else:\n",
    "            #     print(\"Epoch {0} complete in {1:.2f} seconds\".format(j, time2-time1))\n",
    "            if test_data:\n",
    "                correct = self.evaluate(test_data)\n",
    "                accuracy = (correct / n_test) * 100\n",
    "                print(\"Epoch {}, predicted correctly: {:.2f}%\".format(\n",
    "                    j+1, accuracy))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j+1))\n",
    "                \n",
    "    \n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, predicted correctly: 81.63%\n",
      "Epoch 2, predicted correctly: 83.09%\n",
      "Epoch 3, predicted correctly: 83.32%\n",
      "Epoch 4, predicted correctly: 84.45%\n",
      "Epoch 5, predicted correctly: 84.48%\n"
     ]
    }
   ],
   "source": [
    "class NikhilNet(Network):\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return the gradient of the cost function. X represents the input to the network and Y represents the data's labels.\"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Forward pass\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        weighted_inputs = []\n",
    "        activation_derivatives = []\n",
    "        activation_transposes = [x.T]  \n",
    "        \n",
    "        #compute z (weighted inputs to layers), activations, activation derivatives and matrix transposes to reuse later\n",
    "        for layer in range(len(self.weights)):\n",
    "            weighted_input = np.dot(self.weights[layer], activation) + self.biases[layer]\n",
    "            weighted_inputs.append(weighted_input)\n",
    "            activation = sigmoid(weighted_input)\n",
    "            activations.append(activation)\n",
    "            activation_derivatives.append(sigmoid_prime(weighted_input))\n",
    "            activation_transposes.append(activation.T)  # Store transpose\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = self.cost_derivative(activations[-1], y) * activation_derivatives[-1]\n",
    "        \n",
    "        #print(f\"Output layer loss = cost_derivative * sigmoid_prime(weighted_input)\")\n",
    "        \n",
    "        \n",
    "        nabla_b[-1] = loss\n",
    "        nabla_w[-1] = np.dot(loss, activation_transposes[-2])\n",
    "        \n",
    "        #backpropogate loss through network\n",
    "        for layer in range(2, self.num_layers):\n",
    "            loss = np.dot(self.weights[-layer+1].T, loss) * activation_derivatives[-layer]\n",
    "            \n",
    "            #print(f\"\\nLayer {self.num_layers - layer} loss = (weights_{self.num_layers - layer + 1}.T * loss_{self.num_layers - layer + 1}) * sigmoid_prime(weighted_input_{self.num_layers - layer})\")\n",
    "            \n",
    "            \n",
    "            nabla_b[-layer] = loss\n",
    "            nabla_w[-layer] = np.dot(loss, activation_transposes[-layer-1])\n",
    "        \n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "net = NikhilNet([784, 30, 10])\n",
    "\n",
    "net.SGD(training_data, 5, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's analyze the time and space complexity backpropogation algorithm. We'll break it down, section by section:\n",
    "\n",
    "```python\n",
    "def backprop(self, x, y):\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # Forward pass\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        weighted_inputs = []\n",
    "        activation_derivatives = []\n",
    "        activation_transposes = [x.T]  \n",
    "        \n",
    "        #compute z (weighted inputs to layers), activations, activation derivatives and matrix transposes to reuse later\n",
    "        for layer in range(len(self.weights)):\n",
    "            weighted_input = np.dot(self.weights[layer], activation) + self.biases[layer]\n",
    "            weighted_inputs.append(weighted_input)\n",
    "            activation = sigmoid(weighted_input)\n",
    "            activations.append(activation)\n",
    "            activation_derivatives.append(sigmoid_prime(weighted_input))\n",
    "            activation_transposes.append(activation.T) \n",
    "```\n",
    "\n",
    "First, we do a forward pass through the network to initialize the weights and biases, and create two arrays that we can use to hold them. We'll also create arrays to store activations, weighted inputs to neurons, derivatives of the activations, and the transposes of the activations. During the forward pass through the network, all of these things can be calculated and saved for later, since they can be reused. \n",
    "\n",
    "Note that **Dynamic Programming** techniques are in use here. We are using a **bottom-up approach** to pass through the network, breaking the calculation of each layer into a subproblem, and solving them sequentially instead of trying to solve the entire problem at the same time. We are also employing **memoization**, by storing values to reuse later such as activations, derivatives and weighted inputs. Note that this involves a tradeoff between time and space. Computing all of these things on demand would rquire more overall computations (not significant here enough to change our final Big O). By caching these values in arrays, we can retrieve them in $O(1)$ time at the cost of some extra space. Since I know I am implementing this algorithm to use with small network sizes, using less space is appropriate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Backward pass\n",
    "        loss = self.cost_derivative(activations[-1], y) * activation_derivatives[-1]\n",
    "        \n",
    "        #print(f\"Output layer loss = cost_derivative * sigmoid_prime(weighted_input)\")\n",
    "        \n",
    "        \n",
    "        nabla_b[-1] = loss\n",
    "        nabla_w[-1] = np.dot(loss, activation_transposes[-2])\n",
    "        \n",
    "        #backpropogate loss through network\n",
    "        for layer in range(2, self.num_layers):\n",
    "            loss = np.dot(self.weights[-layer+1].T, loss) * activation_derivatives[-layer]\n",
    "            \n",
    "            #print(f\"\\nLayer {self.num_layers - layer} loss = (weights_{self.num_layers - layer + 1}.T * loss_{self.num_layers - layer + 1}) * sigmoid_prime(weighted_input_{self.num_layers - layer})\")\n",
    "            \n",
    "            \n",
    "            nabla_b[-layer] = loss\n",
    "            nabla_w[-layer] = np.dot(loss, activation_transposes[-layer-1])\n",
    "        \n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we define transformations that will turn our image datasets into tensors that pytorch can use, and normalize them, with mean $0.5$ and standard deviation $0.5$, so that values are approximately between the range $[-1,1]$. This is optimal for the allowing the gradient descent algorithm to converge to a true local minimum, and would be very necessary if you were using input features with different scales (though we aren't here).\n",
    "\n",
    "Next, we download training and test datasets, and apply our transformation onto them, and define our batch size. This helps improve computational efficiency by computing the gradient and loss of a batch of weights, and then adjusting the parameters based on a batch, instead of once for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define a Neural Network class, as a child class of PyTorch's nn.Module class. In the initialization, we'll first use the initialization of the parent function, and then create our layers. Each MNIST image is a 28x28 image. After applying the transformation, pytorch flattens this automatically  into a 1D 28*28 = 784 length vector in the input layer. \n",
    "\n",
    "The first hidden layer fc1 thus takes 28*28 input nodes, and outputs 128, which is the input size of the next hidden layer. We then reduce the dimensionality to 64 before passing through the second activation layer. Finally, we have 10 possible outputs, representing one of the ten possible digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create an instance of the neural net class, and then define our loss function and parameter estimation method. Cross entropy loss is a common loss function for categorical data, and stochastic gradient descent is a commonly used algorithm to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train our model over 5 epochs. Notice how the loss decreases with each iteration. This is the network learning in action. The model is using the computed gradient to calculate loss, and then adjust the weights using stochastic gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4260704751565322\n",
      "Epoch 2, Loss: 0.17622237244664607\n",
      "Epoch 3, Loss: 0.12769272889314429\n",
      "Epoch 4, Loss: 0.10216637423484405\n",
      "Epoch 5, Loss: 0.0845510232399331\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # Num of epochs\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test our model's accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 96.68 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test set: {100 * correct / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
