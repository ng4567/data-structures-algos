{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation from Scratch \n",
    "\n",
    "#### Final Project for COMPSCIX404 - Data Structures and Algorithms\n",
    "#### Nikhil Gopal \n",
    "#### August 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will build the backpropogation algorithm, and show how it can be used to compute the gradient of the cost function when training a Neural Network with the Pytorch Deep Learning Framework. Neural Networks trained with backpropogation can be applied to text generation, computer vision, classification, facial recognition, audio generation, speech to text and many more applications.\n",
    "\n",
    "I will start by explaining the algorithm and it's applicability to modern Deep Learning, with an overview of the mathematics behind it. Then, I will analyze the time and space complexity of the algorithm before presenting a code implementation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, every problem in Machine Learning is an **optimization problem**. Optimization is the process of finding the maximum or minimum value of a function, such that it behaves optimally. In the case of Machine Learning, we seek to optimize the *loss function*, which describes the difference between the expected output of an ML model, and it's actual output. For example, if we were predicting human height and our model predicted 100 cm for a person who's true height was 105 cm, the loss would be $105-100=5$ cm. ML models have changable *parameters* whose optimal values can be learned from data such that the model's loss function is as close as possible to zero. \n",
    "\n",
    "\n",
    "Deep Learning is a subsect of Machine Learning that uses a specific type of Machine Learning model called a Multi-Layer Perceptron, also commonly referred to as Neural Networks. Neural Networks pass an input through different \"layers\" of the model, which all contribute to the model's final output. These models often have much higher numbers of learnable parameters than simpler models, but the computations corresponding to a single input must be performed sequentially and cannot be parallelized, since the input of a later layer must be the output of a previous layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input image depicting Neural Net here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many advancements in Machine Learning have come from using Deep Neural Networks (models with lots of layers and parameters) to learn the relevant features of and transform inputted data in novel ways. By leveraging techniques from Calculus, we can compute the gradient vector of a Neural Network's cost function. Each index of the gradient vector contains the partial derivative of the cost function with respect to a parameter, which describes the impact that each individual parameter has on the total loss of the network. The sum of all the loss that comes from every input parameter is the total loss of the network, thus making computing the gradient a key factor in minimizing the loss function.\n",
    "\n",
    "Once the gradient is computed, the model's paramaters are adjusted using a parameter estimation method such as gradient descent, and the loss and parameters are continously adjusted and recomputed until the model's behavior more closely mimics the information it learned from the training data.\n",
    "\n",
    "As model sizes keep increasing, an efficient algorithm is needed to compute the gradient. This algorithm should ideally be computationally efficient so as to minimize total training time and memory usage when training neural networks. This will ensure that models can be deployed into producition faster, and that they can be made more accuracte by being able to be trained on more data in less time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math explaining in detail here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and Practical Example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
